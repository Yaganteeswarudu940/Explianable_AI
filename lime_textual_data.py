# -*- coding: utf-8 -*-
"""lime_textual_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PrjDc52i8lN_1-x7Xe3fWQ89Q1263P5-
"""

#@title Installing LIME
try:
  import lime
except:
  print("Installing LIME")
  !pip install lime

#@title Importing modules
import lime
import sklearn
import numpy as np
from __future__ import print_function
import sklearn
import sklearn.ensemble
import sklearn.metrics
from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import ExtraTreesClassifier

#@title Retrieving newsgroups data
from sklearn.datasets import fetch_20newsgroups
categories = ['sci.electronics', 'sci.space']
newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)
newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)
class_names = ['electronics', 'space']

#@title Vectorizing
vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)
train_vectors = vectorizer.fit_transform(newsgroups_train.data)
test_vectors = vectorizer.transform(newsgroups_test.data)

#@title AutoML experiment: Score measurement variables
best=0 #best classifier score
clf="None" #best classifier name

#@title AutoML experiment: Random forest
rf1 = sklearn.ensemble.RandomForestClassifier(n_estimators=500)
rf1.fit(train_vectors, newsgroups_train.target)
pred = rf1.predict(test_vectors)
score1=sklearn.metrics.f1_score(newsgroups_test.target, pred, average='binary')
if score1>best:
  best=score1
  clf="Random forest"
  print("Random forest has achieved the top score!",score1)
else:
  print("Score of random forest",score1)

#@title AutoML experiment: Bagging
rf2 = BaggingClassifier(KNeighborsClassifier(),n_estimators=500,max_samples=0.5, max_features=0.5)
rf2.fit(train_vectors, newsgroups_train.target)
pred = rf2.predict(test_vectors)
score2=sklearn.metrics.f1_score(newsgroups_test.target, pred, average='binary')
if score2>best:
  best=score2
  clf="Bagging"
  print("Bagging has achieved the top score!",score2)
else:
  print("Score of bagging",score2)

#@title AutoML experiment: Gradient boosting
rf3 = GradientBoostingClassifier(random_state=1, n_estimators=500)
rf3.fit(train_vectors, newsgroups_train.target)
pred = rf3.predict(test_vectors)
score3=sklearn.metrics.f1_score(newsgroups_test.target, pred, average='binary')
if score3>best:
  best=score3
  clf="Gradient boosting"
  print("Gradient boosting has achieved the top score!",score3)
else:
  print("Score of gradient boosting",score3)

#@title AutoML experiment: Decision tree
rf4 = DecisionTreeClassifier(random_state=1)
rf4.fit(train_vectors, newsgroups_train.target)
pred = rf4.predict(test_vectors)
score4=sklearn.metrics.f1_score(newsgroups_test.target, pred, average='binary')
if score4>best:
  best=score4
  clf="Decision tree"
  print("Decision tree has achieved the top score!",score4)
else:
  print("Score of decision tree",score4)

#@title AutoML experiment: Extra trees
rf5 = ExtraTreesClassifier(n_estimators=500,random_state=1)
rf5.fit(train_vectors, newsgroups_train.target)
pred = rf5.predict(test_vectors)
score5=sklearn.metrics.f1_score(newsgroups_test.target, pred, average='binary')
if score5>best:
  best=score5
  clf="Extra trees"
  print("Extra trees has achieved the top score!",score5)
else:
  print("Score of extra trees",score5)

#@title AutoML experiment: Summary
print("The best model is",clf,"with a score of:",round(best,5))
print("Scores:")
print("Random forest:        :",round(score1,5))
print("Bagging               :",round(score2,5))
print("Gradient boosting     :",round(score3,5))
print("Decision tree         :",round(score4,5))
print("Extra trees           :",round(score5,5))

#@title Activate the AutoML mode or choose a classifier in the dropdown list
AutoML= 'On' #@param ["On", "Off"]
dropdown = 'Gradient boosting' #@param ["Random forest", "Bagging", "Gradient boosting","Decision tree", "Extra trees"]

if AutoML=="On":
  dropdown=clf

if clf=="None":
  dropdown="Decision tree"

if dropdown=="Random forest":
  rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)
if dropdown=="Bagging":
  rf = BaggingClassifier(KNeighborsClassifier(),n_estimators=500,max_samples=0.5, max_features=0.5)
if dropdown=="Gradient boosting":
  rf = GradientBoostingClassifier(random_state=1, n_estimators=500)
if dropdown=="Decision tree":
  rf = DecisionTreeClassifier(random_state=1)
if dropdown=="Extra trees":
  rf = ExtraTreesClassifier(random_state=1, n_estimators=500)
 
rf.fit(train_vectors, newsgroups_train.target)

#@title Prediction metrics
pred = rf.predict(test_vectors)
sklearn.metrics.f1_score(newsgroups_test.target, pred, average='binary')

#@title Creating a pipeline with a vectorizer
#Creating a pipeline to implement predictions on raw text lists (sklearn uses vectorized data)
from lime import lime_text
from sklearn.pipeline import make_pipeline
c = make_pipeline(vectorizer, rf)

#@title Predictions
#YOUR INTERCEPTION FUNCTION HERE
newsgroups_test.data[1]="Houston, we have a problem with our ice-cream out here in space. The ice-cream machine is out of order!"
newsgroups_test.data[2]="Why doesn't my TV ever work? It keeps blinking at me as if I were the TV, and it was watching me with a webcam. Maybe AI is becoming autonomous!"
print(newsgroups_test.data[1])
print(c.predict_proba([newsgroups_test.data[1]]))

#@title Creating the LIME explainer
from lime.lime_text import LimeTextExplainer
explainer = LimeTextExplainer(class_names=class_names)
pn=len(newsgroups_test.data)
print("Length of newsgroup",pn)

#@title Selecting a text to explain
index =   5#@param {type: "number"}
idx = index
if idx>pn:
  idx=1

print(newsgroups_test.data[idx])

#@title Generating the explanation
exp = explainer.explain_instance(newsgroups_test.data[idx], c.predict_proba, num_features=10)
print('Document id: %d' % idx)
print('Probability(space) =', c.predict_proba([newsgroups_test.data[idx]])[0,1])
print('True class: %s' % class_names[newsgroups_test.target[idx]])

#@title Explain as a list
exp.as_list()

#@title Removing some features
print('Original prediction:', rf.predict_proba(test_vectors[idx])[0,1])
tmp = test_vectors[idx].copy()
tmp[0,vectorizer.vocabulary_['Posting']] = 0
tmp[0,vectorizer.vocabulary_['Host']] = 0
print('Prediction removing some features:', rf.predict_proba(tmp)[0,1])
print('Difference:', rf.predict_proba(tmp)[0,1] - rf.predict_proba(test_vectors[idx])[0,1])

#@title Explaining with a plot
fig = exp.as_pyplot_figure()

#@title Visual explanations
exp.show_in_notebook(text=False)

#@title Saving explanation
exp.save_to_file('/content/oi.html')

#@title Showing in notebook
exp.show_in_notebook(text=True)